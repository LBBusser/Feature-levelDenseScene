# incontextThesis

This thesis project aims to advance the frontier of visual scene understanding by developing scalable in-context learning frameworks inspired by cutting-edge methodologies in the field. The project is motivated by works such as [1] which emphasizes the significance of contextual information in interpreting visual data, and [2] which addresses the challenges, using NLP techniques, of learning scalability in large vision models.
Drawing from the strengths of state-of-the-art in-context visual prompts research, the project seeks to investigate methods that enable models to efficiently interpret and analyze visual scenes. The core of the investigation revolves around the following objectives:
1. In-Context Learning in various domains: Investigate in-context learning paradigms that allow models to adapt to new visual scenarios with minimal training. Evaluate the effectiveness of visual prompts and memory-augmented models in enabling rapid context-based adaptation, crucial for real-world applications where learning from few examples is useful. And apply these methodologies to diverse scene understanding tasks such as semantic segmentation, object recognition, and depth estimation. Analyze the model's versatility and robustness across different domains and datasets.
2. Cross-Attention Mechanisms: One such approach uses advanced cross-attention models that make use of a memory bank of pre-encoded features to provide contextually rich scene interpretations. By focusing on relevant features across a dataset, the aim is to enhance the model's ability to make accurate local predictions that are globally coherent and therefore show hints of scene and spatial understanding capabilities.
3. Investigate sequential modeling on pretrained models: Explore the scalability and efficiency of visual in-context learning models through the application of sequential modeling techniques, as inspired by the research conducted by [2]. Unlike the original study, which involved the creation of new models (via pre-training), this thesis focuses more on implementing these advanced techniques on existing, off-the-shelf models. The core objective is to validate whether the principles of sequential modeling, as detailed in the cited research, can be effectively adapted to pre-existing large vision models to enhance their performance.
Together, these three focal points aim to create a framework where existing large vision models can be significantly enhanced, showcasing improved context-awareness, adaptability, and scalability. This approach not only tries to support the research of current vision-language model capabilities but also bridges the gap between theoretical research and practical, real-world applications.

[1] Balažević, I., Steiner, D., Parthasarathy, N., Arandjelović, R., & Hénaff, O. J. (2023). Towards In-context Scene Understanding. arXiv preprint arXiv:2306.01667.
[2] Bai, Y., Geng, X., Mangalam, K., Bar, A., Yuille, A., Darrell, T., ... & Efros, A. A. (2023). Sequential modeling enables scalable learning for large vision models. arXiv preprint arXiv:2312.00785.
